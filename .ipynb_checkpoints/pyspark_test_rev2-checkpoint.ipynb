{
 "metadata": {
  "name": "",
  "signature": "sha256:f2030e570157cb26ea353b10b7efa5296ccadfc3871b4dfee35cfb2c3e7cde9b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "spark_home = \"/usr/local/share/spark-0.9.0-incubating-bin-hadoop2\"\n",
      "os.environ['SPARK_HOME'] = spark_home\n",
      "import sys\n",
      "path_to_pyspark = os.path.join(spark_home,\"python\")\n",
      "sys.path.insert(0,path_to_pyspark)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getTimelyFilesFromHDFS(startTime, timeWindowInMinutes, measDirName):\n",
      "\thadoopCommand = 'hadoop fs -ls %s/' % measDirName\n",
      "\tfullListings = os.popen(hadoopCommand).read().split('\\n')\n",
      "\tfileNames = [x.split(' ')[-1].split('/')[-1] for x in fullListings][1:-1]\n",
      "\ttimelyFiles = []\n",
      "\tdtStartTime = dt.datetime.strptime(startTime, DATETIME_FORMAT_PCMD_FILENAME)\n",
      "\tdtEndTime = dtStartTime + dt.timedelta(0, timeWindowInMinutes * 60)\n",
      "\tfor fn in fileNames:\n",
      "\t\tfileCreateTimeString = '.'.join(fn.split('.')[:2])\n",
      "\t\tdtFileCreateTime = dt.datetime.strptime(fileCreateTimeString, DATETIME_FORMAT_PCMD_FILENAME)\n",
      "\t\tif ((dtFileCreateTime >= dtStartTime) and (dtFileCreateTime <= dtEndTime)):\n",
      "\t\t\tif (measDirName[-1] != '/'):\n",
      "\t\t\t\tsep = '/'\n",
      "\t\t\telse:\n",
      "\t\t\t\tsep = ''\n",
      "\t\t\ttimelyFiles.append(measDirName + sep + fn)\n",
      "\t\t\n",
      "\treturn timelyFiles"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def mapRealPCMDString(pcmdString):\n",
      "    servingCell, fieldDict = getFieldDictFromMeasRecord(pcmdString)\n",
      "    #debug\n",
      "    if not fieldDict:\n",
      "\t\treturn []\n",
      "# Reset listOfMeasurements\n",
      "    listOfMeasurements = []\n",
      "    if servingCell in fieldDict.keys():\n",
      "\t\tserverDLRLC = fieldDict[servingCell][0][0]\n",
      "    else:\n",
      "\t\tserverDLRLC = -1\n",
      "    print \"serverDLRC =\",  serverDLRLC\n",
      "    \n",
      "    for cellID in fieldDict.keys():\n",
      "        f1 = fieldDict[cellID][0][0]\n",
      "        f2 = fieldDict[cellID][0][1]\n",
      "        listOfMeasurements.append(((cellID), \\\n",
      "\t\t\t(f1,f1*f1,serverDLRLC,serverDLRLC*serverDLRLC,f1*serverDLRLC,1)\t\\\n",
      "\t\t\t))\n",
      "\treturn listOfMeasurements"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "this function retrieves all measurement strings from a pcmd string \n",
      "\"\"\"\n",
      "def getListOfMeasurementRecordsFromPCMDString(pcmdString):\n",
      "\telementsOfPCMDString = pcmdString.split(\";\")\n",
      "\tservingCell = elementsOfPCMDString[FIELD_SERVING_CELL_ID_PRIMARY - 1]\n",
      "\t#RSRP is just one field needed.\n",
      "\tif START_CHAR_MEAS not in pcmdString:\n",
      "\t\treturn servingCell, []\n",
      "\n",
      "\tstartOfMeasRecords = pcmdString.index(START_CHAR_MEAS) + 1\n",
      "\tendOfMeasRecords = pcmdString.index(END_CHAR_PCMD)\n",
      "\tallMeasRecords = (pcmdString[startOfMeasRecords : endOfMeasRecords]).split(\";\")\n",
      "\tlistOfMeasRecords = []\n",
      "\tcount = 0\n",
      "\twhile (count < len(allMeasRecords)):\n",
      "\t\tstartOfThisMeasRecord = count\n",
      "\t\tlengthOfThisMeasRecord = int(allMeasRecords[count]) * LENGTH_MEAS_DATA + 1\n",
      "\t\tendOfThisMeasRecord = startOfThisMeasRecord + lengthOfThisMeasRecord\n",
      "\t\tlistOfMeasRecords.append(allMeasRecords[startOfThisMeasRecord : endOfThisMeasRecord])\n",
      "\t\tcount = endOfThisMeasRecord\n",
      "\t#Add an additional return value that will include additional PCMD fields\n",
      "\t#Need to correct the syntax to combine these two structures into the listOfllMeasRecords\n",
      "\treturn servingCell, listOfMeasRecords"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "The following function reads a pcmd measurement string and retrieves the values in\n",
      "a dictionary format with keys being cellids and values being RSRPs \n",
      "\"\"\"\n",
      "def getRSRPDictFromMeasRecord(servingCell, measRecord):\n",
      "\tnumCellsInMeasRecord = int(measRecord[0])\n",
      "\tnumMeasData = int((len(measRecord) - 1)/ LENGTH_MEAS_DATA)\n",
      "\t\"\"\" create a dictionary with rsrp measurements in this record \"\"\"\n",
      "\tindex = 0\n",
      "\trsrpMeasVal = defaultdict(float) \n",
      "\n",
      "\tfor count in range(0, numMeasData):\n",
      "\t\tthisMeasData = measRecord[1+count*LENGTH_MEAS_DATA : 1+(count + 1)*LENGTH_MEAS_DATA]\n",
      "\t\tthisCell = thisMeasData[FIELD_CELL_ID_MEAS - 1]\n",
      "\t\t#Apparently the RSRP was stored as a string as well and needs to be converted\n",
      "\t\tthisRSRPString = thisMeasData[FIELD_RSRP_MEAS - 1]\n",
      "\t\tif (thisCell == '') or (thisRSRPString == ''):\n",
      "\t\t\tcontinue\n",
      "\t\telse:\n",
      "\t\t\t#This line creates a dictionary based on the cell found in the measurement data. \n",
      "\t\t\trsrpMeasVal[thisCell] = float(thisRSRPString)\n",
      "\t#Note servingCell was passed to this function but not used.\n",
      "\treturn rsrpMeasVal"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "define a commutative and associative function for combining aggregate statistics\n",
      "this is used by reduce function in the code later\n",
      "\"\"\"\n",
      "def reduceToSingleStat(stat1, stat2):\n",
      "\tsumMeasCount = stat1[NUM_MEAS_STATISTICS-1] + stat2[NUM_MEAS_STATISTICS-1]\n",
      "\twtStat1 = 1.0 * stat1[NUM_MEAS_STATISTICS-1] / sumMeasCount\n",
      "\twtStat2 = 1.0 * stat2[NUM_MEAS_STATISTICS-1] / sumMeasCount\n",
      "\treducedStat = [0] * NUM_MEAS_STATISTICS\n",
      "\tfor index in range(0, NUM_MEAS_STATISTICS - 1):\n",
      "\t\treducedStat[index] = wtStat1 * stat1[index] + wtStat2 * stat2[index]\n",
      "\treducedStat[NUM_MEAS_STATISTICS-1] = sumMeasCount\n",
      "\treturn tuple(reducedStat)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def toCovarMatrix(stat):\n",
      "\tparams = [0] * NUM_MEAS_STATISTICS\n",
      "\tfor index in [0, 1, NUM_MEAS_STATISTICS-1]:\n",
      "\t\tparams[index] = stat[index]\n",
      "\tparams[2] = stat[2] - stat[0]*stat[0]\n",
      "\tparams[3] = stat[3] - stat[0]*stat[1]\n",
      "\tparams[4] = stat[4] - stat[1]*stat[1]\n",
      "\treturn tuple(params)\n",
      "\n",
      "\n",
      "def saveAsFile(outputFile, cellStats, caseId, rnpId):\n",
      "\twith open(outputFile + '.txt', 'w') as opf:\n",
      "\t\tfor cs in cellStats:\n",
      "\t\t\top_string = '{:16s}  {:16s}   '.format(cs[0][0], cs[0][1])\n",
      "\t\t\tfor i in range(0, NUM_MEAS_STATISTICS - 1):\n",
      "\t\t\t\top_string =  op_string + '{:>10s}'.format('{:4.2f}'.format(cs[1][i]))\n",
      "\t\t\top_string =  op_string +\\\n",
      "\t\t\t\t '{:>8s}'.format('{:4d}'.format(cs[1][NUM_MEAS_STATISTICS-1]))\n",
      "\t\t\topf.write(op_string + '\\n')\n",
      "\n",
      "\n",
      "\tallDBRowsAsStrings = getAllDBRowsAsStrings(cellStats, caseId, rnpId)\n",
      "\twith open(outputFile + '.csv', 'w') as opf:\n",
      "\t\tfor measString in allDBRowsAsStrings:\n",
      "\t\t\topf.write(measString + '\\n')\n",
      "\n",
      "\treturn\n",
      "\n",
      "\n",
      "def getAllDBRowsAsStrings(cellStats, caseId, rnpId):\n",
      "\tallDBRowsAsStrings = []\n",
      "\tfor cs in cellStats:\n",
      "\t\tcommonString = \"%d, %d\" % (caseId, rnpId)\n",
      "\t\tmeasStats = [RSRP_MIN + cs[1][0], RSRP_MIN + cs[1][1], cs[1][2], cs[1][3], cs[1][4], cs[1][5]]\n",
      "\t\tfor i in range(len(MEAS_STRINGS)):\n",
      "\t\t\tmeasType = MEAS_STRINGS[i]\n",
      "\t\t\tmeasuredCells= (cs[0][0], cs[0][1])\n",
      "\t\t\tmeasString = \"%s, '%s', '%s', '%s', '%s', %s, %s\"\\\n",
      "\t\t\t             % (commonString, measType, cs[0][0], measuredCells[0], measuredCells[1],\n",
      "\t\t\t                str(measStats[i]), str(measStats[5]))\n",
      "\t\t\tallDBRowsAsStrings.append('(' + measString + ')')\n",
      "\treturn allDBRowsAsStrings\n",
      "\n",
      "\n",
      "def saveToDatabase(cellStats, caseId, rnpId):\n",
      "\tconStr = getDBConnStringConfig()\n",
      "\tcnx = pyodbc.connect(conStr)\n",
      "\tcursor = cnx.cursor()\n",
      "\tcnf = ConfigParser.ConfigParser()\n",
      "\tcnf.read(CONFIG_FILE)\n",
      "\ttableName = dict(cnf.items('DB_TABLE'))['name']\n",
      "\ttableOverwrite = dict(cnf.items('DB_TABLE'))['overwrite']\n",
      "\tif tableOverwrite == 'y':\n",
      "\t\tdelStr = 'DELETE FROM %s WHERE Case_Id = %d AND RNP_Simulation_Id = %d' % (tableName, caseId, rnpId)\n",
      "\t\tcursor.execute(delStr)\n",
      "\t\tprint ('Deleted %d from DB: ' % cursor.rowcount) + delStr\n",
      "\t\tcnx.commit()\n",
      "\n",
      "\n",
      "\tallDBRowsAsStrings = getAllDBRowsAsStrings(cellStats, caseId, rnpId)\n",
      "\tfor dbRowString in allDBRowsAsStrings:\n",
      "\t\tinsertStr = \"INSERT INTO RSRPJointGaussianParams VALUES %s\" % dbRowString\n",
      "\t\tcursor.execute(insertStr)\n",
      "\tcnx.commit()\n",
      "\n",
      "\tcnx.close()\n",
      "\treturn"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getDBConnStringConfig():\n",
      "\tconStr = r''\n",
      "\tcnf = ConfigParser.ConfigParser()\n",
      "\tcnf.read(CONFIG_FILE)\n",
      "\tfor option in cnf.items('DATABASE'):\n",
      "\t\tconStr += '%s=%s;' %  option\n",
      "\treturn conStr.rstrip(';')\n",
      "\n",
      "\n",
      "def getBasicConfigInfo():\n",
      "\tcnf = ConfigParser.ConfigParser()\n",
      "\tcnf.read(CONFIG_FILE)\n",
      "\tisRealPCMD = cnf.getint('BASIC', 'REAL_PCMD')\n",
      "\tsparkMaster = cnf.get('BASIC', 'SPARK_MASTER')\n",
      "\tmeasDir = cnf.get('BASIC', 'HADOOP_PCMD_DIR')\n",
      "\tcaseId = cnf.getint('BASIC', 'CASE_ID')\n",
      "\trnpId = cnf.getint('BASIC', 'RNP_SIM_ID')\n",
      "\treturn isRealPCMD, sparkMaster, measDir, caseId, rnpId\n",
      "\n",
      "\n",
      "def createClusters(cellStats):\n",
      "\tcellGraph = nx.DiGraph()\n",
      "\tcellNodes =set([])\n",
      "\tcellEdges = []\n",
      "\tfor cs in cellStats:\n",
      "\t\tcellNodes = cellNodes.union(set(cs[0]))\n",
      "\n",
      "\tcellTotalWeight = defaultdict(int)\n",
      "\tfor node in cellNodes:\n",
      "\t\tcellTotalWeight[node] = sum([x[1][5] for x in cellStats if x[0][0] == node and x[0][1] != node])\n",
      "\t\t#print '%s : %d' % (node, cellTotalWeight[node])\n",
      "\t\t#sys.stdin.read(1)\n",
      "\n",
      "\tfor cs in cellStats:\n",
      "\t\tif ((cs[0][0] != cs[0][1]) and (cs[1][5] > cellTotalWeight[cs[0][0]] * INT_THRESHOLD)):\n",
      "\t\t\tcellEdges.append(cs[0])\n",
      "\t\t\t#cellEdges.append((cs[0][0], cs[0][1], cs[1][5]))\n",
      "\n",
      "\tcellGraph.add_nodes_from(list(cellNodes))\n",
      "\tcellGraph.add_edges_from(cellEdges)\n",
      "\tcomps = nx.connected_component_subgraphs(cellGraph.to_undirected())\n",
      "\tprint 'Nodes = %d, Edges = %d' % (cellGraph.number_of_nodes(), cellGraph.number_of_edges())\n",
      "\tind = 0\n",
      "\tfor sg in comps:\n",
      "\t\tind += 1\n",
      "\t\tprint '%d : %d , %d' % (ind, sg.number_of_nodes(), sg.number_of_edges())\n",
      "\n",
      "\treturn"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def computeCellStatsForOneMeasFile(sparkMaster, isRealPCMD, measFileName):\n",
      "\tsparkAppName = 'PCMDMiner' + uuid.uuid1().hex\n",
      "\tsparkConf = (SparkConf()\n",
      "\t       .setMaster(sparkMaster)\n",
      "    \t       .setAppName(sparkAppName)\n",
      "    \t       .set('spark.scheduler.mode', 'FAIR'))\n",
      "\n",
      "\t#sc = SparkContext(conf = sparkConf)\n",
      "\tpcmdStrings = sc.textFile(measFileName)\n",
      "#\tcellStats = pcmdStrings\\\n",
      "#\t            .flatMap(lambda x: mapPCMDString(isRealPCMD, x)) \\\n",
      "#\t            .reduceByKey(lambda x, y: reduceToSingleStat(x, y)) \\\n",
      "#\t            .collect()\n",
      "\tcellStats = pcmdStrings\\\n",
      "\t\t.flatMap(lambda x: mapPCMDString(isRealPCMD, x)) \\\n",
      "\t    .reduceByKey(lambda x, y: reduceToSingleStat(x, y)) \\\n",
      "\t\t.collect()\n",
      "\tsc.stop()\n",
      "\t#debug\n",
      "\t            \n",
      "\treturn cellStats"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "this function gets each field requested from the pcmd file\n",
      "\"\"\"\n",
      "def getFieldDictFromMeasRecord(pcmdString):\n",
      "    #assume this function is getting one record at a time\n",
      "    elementsOfPCMDString = pcmdString.split(\";\")\n",
      "    cellID = elementsOfPCMDString[FIELD_CELL_ID -1]\n",
      "    servingCell = cellID\n",
      "    secondaryFieldNum = elementsOfPCMDString[173-1]\n",
      "    if secondaryFieldNum == 2:\n",
      "        DLRLC_First = elementsOfPCMDString[251-1]\n",
      "    else:\n",
      "        DLRLC_First = []\n",
      "        return servingCell, []\n",
      " \n",
      "    fieldDict =defaultdict(list)\n",
      "    fieldDict[cellID].append((float(DLRLC_First),servingCell))\n",
      "    return servingCell, fieldDict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "startTime = '2014-05-12.12_00'\n",
      "timeWindowInMinutes = 2\n",
      "print startTime, timeWindowInMinutes\n",
      "\n",
      "\n",
      "if True:\n",
      "    \n",
      "    isRealPCMD, sparkMaster, measDir, caseId, rnpId = getBasicConfigInfo() \n",
      " \n",
      "    \n",
      "    print sparkMaster\n",
      "    \n",
      "    cellStats = computeCellStatsForManyMeasFiles(startTime, timeWindowInMinutes, isRealPCMD, sparkMaster, measDir)\n",
      "    #saveAsFile(\"output_cell_statistics\", cellStats, caseId, rnpId)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2014-05-12.12_00 2\n",
        "spark://192.168.122.1:7077\n"
       ]
      },
      {
       "ename": "Py4JJavaError",
       "evalue": "An error occurred while calling o39.collect.\n: org.apache.spark.SparkException: Job aborted: Task 1.0:0 failed 1 times (most recent failure: Exception failure: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/share/spark-0.9.0-incubating-bin-hadoop2/python/pyspark/worker.py\", line 77, in main\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/share/spark-0.9.0-incubating-bin-hadoop2/python/pyspark/rdd.py\", line 1022, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/share/spark-0.9.0-incubating-bin-hadoop2/python/pyspark/rdd.py\", line 1022, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/share/spark-0.9.0-incubating-bin-hadoop2/python/pyspark/rdd.py\", line 194, in func\n    def func(s, iterator): return f(iterator)\n  File \"/usr/local/share/spark-0.9.0-incubating-bin-hadoop2/python/pyspark/rdd.py\", line 870, in combineLocally\n    for x in iterator:\n  File \"<ipython-input-5-5279766f019d>\", line 14, in <lambda>\n  File \"<ipython-input-2-bd9b475b5651>\", line 24, in mapPCMDString\n  File \"<ipython-input-3-2901712d7d39>\", line 2, in mapRealPCMDString\n  File \"<ipython-input-6-299d01451354>\", line 9, in getFieldDictFromMeasRecord\nIndexError: list index out of range\n)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1028)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1026)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$abortStage(DAGScheduler.scala:1026)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:619)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:619)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:619)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$start$1$$anon$2$$anonfun$receive$1.applyOrElse(DAGScheduler.scala:207)\n\tat akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)\n\tat akka.actor.ActorCell.invoke(ActorCell.scala:456)\n\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:219)\n\tat akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-7-a73183530eab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[0msparkMaster\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mcellStats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomputeCellStatsForManyMeasFiles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstartTime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeWindowInMinutes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0misRealPCMD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparkMaster\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeasDir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[1;31m#saveAsFile(\"output_cell_statistics\", cellStats, caseId, rnpId)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-1-d4d229490bc0>\u001b[0m in \u001b[0;36mcomputeCellStatsForManyMeasFiles\u001b[1;34m(startTime, timeWindowInMinutes, isRealPCMD, sparkMaster, measDir)\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[0mlistCellStats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtimelyFiles\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m                 \u001b[0mcs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomputeCellStatsForOneMeasFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparkMaster\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0misRealPCMD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m                 \u001b[0mlistCellStats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0msparkAppName\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'PCMDMiner'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-5-5279766f019d>\u001b[0m in \u001b[0;36mcomputeCellStatsForOneMeasFile\u001b[1;34m(sparkMaster, isRealPCMD, measFileName)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m#                   .reduceByKey(lambda x, y: reduceToSingleStat(x, y)) \\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m#                   .collect()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mcellStats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpcmdStrings\u001b[0m         \u001b[1;33m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmapPCMDString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0misRealPCMD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m            \u001b[1;33m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mreduceToSingleStat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m                 \u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;31m#debug\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/share/spark-0.9.0-incubating-bin-hadoop2/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    461\u001b[0m         \"\"\"\n\u001b[0;32m    462\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0m_JavaStackTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mst\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 463\u001b[1;33m           \u001b[0mbytesInJava\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    464\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_collect_iterator_through_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbytesInJava\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/share/spark-0.9.0-incubating-bin-hadoop2/python/lib/py4j-0.8.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    535\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[1;32m--> 537\u001b[1;33m                 self.target_id, self.name)\n\u001b[0m\u001b[0;32m    538\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/share/spark-0.9.0-incubating-bin-hadoop2/python/lib/py4j-0.8.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    299\u001b[0m                     \u001b[1;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[0;32m    301\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                 raise Py4JError(\n",
        "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o39.collect.\n: org.apache.spark.SparkException: Job aborted: Task 1.0:0 failed 1 times (most recent failure: Exception failure: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/share/spark-0.9.0-incubating-bin-hadoop2/python/pyspark/worker.py\", line 77, in main\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/share/spark-0.9.0-incubating-bin-hadoop2/python/pyspark/rdd.py\", line 1022, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/share/spark-0.9.0-incubating-bin-hadoop2/python/pyspark/rdd.py\", line 1022, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/share/spark-0.9.0-incubating-bin-hadoop2/python/pyspark/rdd.py\", line 194, in func\n    def func(s, iterator): return f(iterator)\n  File \"/usr/local/share/spark-0.9.0-incubating-bin-hadoop2/python/pyspark/rdd.py\", line 870, in combineLocally\n    for x in iterator:\n  File \"<ipython-input-5-5279766f019d>\", line 14, in <lambda>\n  File \"<ipython-input-2-bd9b475b5651>\", line 24, in mapPCMDString\n  File \"<ipython-input-3-2901712d7d39>\", line 2, in mapRealPCMDString\n  File \"<ipython-input-6-299d01451354>\", line 9, in getFieldDictFromMeasRecord\nIndexError: list index out of range\n)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1028)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1026)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$abortStage(DAGScheduler.scala:1026)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:619)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:619)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:619)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$start$1$$anon$2$$anonfun$receive$1.applyOrElse(DAGScheduler.scala:207)\n\tat akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)\n\tat akka.actor.ActorCell.invoke(ActorCell.scala:456)\n\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:219)\n\tat akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "print listCellStats"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'listCellStats' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-8-0d031394bef5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0mlistCellStats\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mNameError\u001b[0m: name 'listCellStats' is not defined"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "listOfMeasurements"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'listOfMeasurements' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-10-783123a9f5c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlistOfMeasurements\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mNameError\u001b[0m: name 'listOfMeasurements' is not defined"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}