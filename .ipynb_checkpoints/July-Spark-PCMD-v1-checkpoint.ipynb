{
 "metadata": {
  "name": "",
  "signature": "sha256:817230fcfd63214017210353b61508f2f5076ea0d8b3258abd22758736264070"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#%load  \"gen_cell.py\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Created by Supratim Deb on March 20, 2014\n",
      "\"\"\"\n",
      "This is a pyspark code for generating cell statistics from PCMD data\n",
      "\"\"\"\n",
      "\n",
      "from pyspark import SparkContext, SparkConf\n",
      "from collections import defaultdict\n",
      "import sys, operator, math, imp, os, uuid, ConfigParser, pyodbc\n",
      "import datetime as dt\n",
      "from multiprocessing import Pool\n",
      "import itertools as it\n",
      "import threading\n",
      "import networkx as nx\n",
      "import matplotlib . pyplot as plt\n",
      "\n",
      "#Max cores per worker\n",
      "SPARK_CORES_MAX = 3\n",
      "\n",
      "\n",
      "#Constants for synthetic pcmd string\n",
      "STRING_INDEX_FIRST_CELL = 4\n",
      "STRING_INDEX_NUM_CELLS = 3\n",
      "NUM_MEAS_STATISTICS = 6\n",
      "\n",
      "#Constants for real pcmd string\n",
      "CONFIG_FILE = 'config.ini'\n",
      "FIELD_SERVING_CELL_ID_PRIMARY = 21\n",
      "FIELD_UE_LOCATION_CAPABILITY = 158\n",
      "FIELD_CELL_ID = 131\n",
      "START_CHAR_MEAS = '['\n",
      "END_CHAR_PCMD = '|'\n",
      "LENGTH_MEAS_DATA = 4\n",
      "FIELD_CELL_ID_MEAS = 2\n",
      "FIELD_RSRP_MEAS = 3\n",
      "RSRP_MIN = -140.0\n",
      "DATETIME_FORMAT_PCMD_FILENAME = '%Y-%m-%d.%H_%M'\n",
      "INT_THRESHOLD = 0.09\n",
      "\n",
      "\n",
      "#Constant strings for outpul file string\n",
      "MEAS_STRINGS = ['MeanServer', 'MeanNeighbor', 'VarianceServer', 'CovarianceNeighbor', 'VarianceNeighbor']\n",
      "STRING_AVG_SERVER_RSRP = 'MeanServer'\n",
      "STRING_VAR_SERVER_RSRP = 'VarianceServer'\n",
      "STRING_AVG_NGHBR_RSRP = 'MeanNeighbor'\n",
      "STRING_VAR_NGHBR_RSRP = 'VarianceNeighbor'\n",
      "STRING_COVAR_RSRP = 'CovarianceNeighbor'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getTimelyFilesFromHDFS(startTime, timeWindowInMinutes, measDirName):\n",
      "\thadoopCommand = 'hadoop fs -ls %s/' % measDirName\n",
      "\tfullListings = os.popen(hadoopCommand).read().split('\\n')\n",
      "\tfileNames = [x.split(' ')[-1].split('/')[-1] for x in fullListings][1:-1]\n",
      "\ttimelyFiles = []\n",
      "\tdtStartTime = dt.datetime.strptime(startTime, DATETIME_FORMAT_PCMD_FILENAME)\n",
      "\tdtEndTime = dtStartTime + dt.timedelta(0, timeWindowInMinutes * 60)\n",
      "\tfor fn in fileNames:\n",
      "\t\tfileCreateTimeString = '.'.join(fn.split('.')[:2])\n",
      "\t\tdtFileCreateTime = dt.datetime.strptime(fileCreateTimeString, DATETIME_FORMAT_PCMD_FILENAME)\n",
      "\t\tif ((dtFileCreateTime >= dtStartTime) and (dtFileCreateTime <= dtEndTime)):\n",
      "\t\t\tif (measDirName[-1] != '/'):\n",
      "\t\t\t\tsep = '/'\n",
      "\t\t\telse:\n",
      "\t\t\t\tsep = ''\n",
      "\t\t\ttimelyFiles.append(measDirName + sep + fn)\n",
      "\t\t\n",
      "\treturn timelyFiles\n",
      "\n",
      "\n",
      "\n",
      "def mapPCMDString(isRealPCMD, pcmdString):\n",
      "\tif isRealPCMD:\n",
      "\t\treturn mapRealPCMDString(pcmdString)\n",
      "\telse:\n",
      "\t\treturn mapSyntheticPCMDString(pcmdString)\n",
      "\n",
      "\n",
      "def mapRealPCMDString(pcmdString):\n",
      "\tservingCell, fieldDict = getFieldDictFromMeasRecord(pcmdString)\n",
      "\t#debug\n",
      "\tif not fieldDict:\n",
      "\t\treturn []\n",
      "\n",
      "\t# Reset listOfMeasurements\n",
      "\tlistOfMeasurements = []\n",
      "\tif servingCell in fieldDict.keys():\n",
      "\t\tserverDLRLC = fieldDict[servingCell][0][0]\n",
      "\telse:\n",
      "\t\tserverDLRLC = -1\n",
      "\n",
      "\tfor cellID in fieldDict.keys():\n",
      "\t\tf1 = fieldDict[cellID][0][0]\n",
      "\t\tf2 = fieldDict[cellID][0][1]\n",
      "\t\tlistOfMeasurements.append((\n",
      "\t\t\t(servingCell, cellID), \\\n",
      "\t\t\t(f1,f1*f1,serverDLRLC,serverDLRLC*serverDLRLC,f1*serverDLRLC,1)\t\n",
      "\t\t\t))\n",
      "\treturn listOfMeasurements\n",
      "\n",
      "\n",
      "\"\"\"\n",
      "this function gets each field requested from the pcmd file\n",
      "\"\"\"\n",
      "def getFieldDictFromMeasRecord(pcmdString):\n",
      "\t#assume this function is getting one record at a time\n",
      "\telementsOfPCMDString = pcmdString.split(\";\")\n",
      "\tif len(elementsOfPCMDString) < 255:\n",
      "\t\treturn -1, []\n",
      "\tcellID = elementsOfPCMDString[FIELD_CELL_ID -1]\n",
      "\tservingCell = cellID\n",
      "\tsecondaryFieldNum = elementsOfPCMDString[173-1]\n",
      "\tif secondaryFieldNum >= 1 and len(elementsOfPCMDString) >= 299:\n",
      "\t\tDLRLC_First = elementsOfPCMDString[251-1]\n",
      "\telse:\n",
      "\t\tDLRLC_First = []\n",
      "\t        return servingCell, []\n",
      "\n",
      "\tfieldDict =defaultdict(list)\n",
      "\n",
      "\tfieldDict[cellID].append((float(DLRLC_First),servingCell))\n",
      "\n",
      "\treturn servingCell, fieldDict\n",
      "\n",
      "\"\"\"\n",
      "define a commutative and associative function for combining aggregate statistics\n",
      "this is used by reduce function in the code later\n",
      "\"\"\"\n",
      "def reduceToSingleStat(stat1, stat2):\n",
      "\tif stat2.share[1] < NUM_MEAS_STATISTICS -1:\n",
      "\t\treturn []\n",
      "\tsumMeasCount = stat1[NUM_MEAS_STATISTICS-1] + stat2[NUM_MEAS_STATISTICS-1]\n",
      "\twtStat1 = 1.0 * stat1[NUM_MEAS_STATISTICS-1] / sumMeasCount\n",
      "\twtStat2 = 1.0 * stat2[NUM_MEAS_STATISTICS-1] / sumMeasCount\n",
      "\treducedStat = [0] * NUM_MEAS_STATISTICS\n",
      "\tfor index in range(0, NUM_MEAS_STATISTICS - 1):\n",
      "\t\treducedStat[index] = wtStat1 * stat1[index] + wtStat2 * stat2[index]\n",
      "\treducedStat[NUM_MEAS_STATISTICS-1] = sumMeasCount\n",
      "\treturn tuple(reducedStat)\n",
      "\n",
      "\n",
      "def getDBConnStringConfig():\n",
      "\tconStr = r''\n",
      "\tcnf = ConfigParser.ConfigParser()\n",
      "\tcnf.read(CONFIG_FILE)\n",
      "\tfor option in cnf.items('DATABASE'):\n",
      "\t\tconStr += '%s=%s;' %  option\n",
      "\treturn conStr.rstrip(';')\n",
      "\n",
      "\n",
      "def getBasicConfigInfo():\n",
      "\tcnf = ConfigParser.ConfigParser()\n",
      "\tcnf.read(CONFIG_FILE)\n",
      "\tisRealPCMD = cnf.getint('BASIC', 'REAL_PCMD')\n",
      "\tsparkMaster = cnf.get('BASIC', 'SPARK_MASTER')\n",
      "\tmeasDir = cnf.get('BASIC', 'HADOOP_PCMD_DIR')\n",
      "\tcaseId = cnf.getint('BASIC', 'CASE_ID')\n",
      "\trnpId = cnf.getint('BASIC', 'RNP_SIM_ID')\n",
      "\treturn isRealPCMD, sparkMaster, measDir, caseId, rnpId\n",
      "\n",
      "\n",
      "def createClusters(cellStats):\n",
      "\tcellGraph = nx.DiGraph()\n",
      "\tcellNodes =set([])\n",
      "\tcellEdges = []\n",
      "\tfor cs in cellStats:\n",
      "\t\tcellNodes = cellNodes.union(set(cs[0]))\n",
      "\n",
      "\tcellTotalWeight = defaultdict(int)\n",
      "\tfor node in cellNodes:\n",
      "\t\tcellTotalWeight[node] = sum([x[1][5] for x in cellStats if x[0][0] == node and x[0][1] != node])\n",
      "\t\t#print '%s : %d' % (node, cellTotalWeight[node])\n",
      "\t\t#sys.stdin.read(1)\n",
      "\n",
      "\tfor cs in cellStats:\n",
      "\t\tif ((cs[0][0] != cs[0][1]) and (cs[1][5] > cellTotalWeight[cs[0][0]] * INT_THRESHOLD)):\n",
      "\t\t\tcellEdges.append(cs[0])\n",
      "\t\t\t#cellEdges.append((cs[0][0], cs[0][1], cs[1][5]))\n",
      "\n",
      "\tcellGraph.add_nodes_from(list(cellNodes))\n",
      "\tcellGraph.add_edges_from(cellEdges)\n",
      "\tcomps = nx.connected_component_subgraphs(cellGraph.to_undirected())\n",
      "\tprint 'Nodes = %d, Edges = %d' % (cellGraph.number_of_nodes(), cellGraph.number_of_edges())\n",
      "\tind = 0\n",
      "\tfor sg in comps:\n",
      "\t\tind += 1\n",
      "\t\tprint '%d : %d , %d' % (ind, sg.number_of_nodes(), sg.number_of_edges())\n",
      "\n",
      "\treturn"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def computeCellStatsForOneMeasFile(sparkMaster, isRealPCMD, measFileName):\n",
      "    sparkAppName = 'PCMDMiner' + uuid.uuid1().hex\n",
      "    sparkConf = (SparkConf()\n",
      "                 .setMaster(sparkMaster)\n",
      "                 .setAppName(sparkAppName)\n",
      "                 .set('spark.scheduler.mode', 'FAIR'))\n",
      "    sc = SparkContext(conf = sparkConf)\n",
      "    pcmdStrings = sc.textFile(measFileName)\n",
      "    cellStats = pcmdStrings\\\n",
      "        .flatMap(lambda x: mapPCMDString(isRealPCMD, x)) \\\n",
      "        .reduceByKey(lambda x, y: reduceToSingleStat(x, y)) \\\n",
      "        .collect()\n",
      "    sc.stop()\n",
      "    #debug\n",
      "    \n",
      "    return cellStats"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def computeCellStatsForManyMeasFiles(startTime, timeWindowInMinutes, isRealPCMD, sparkMaster, measDir):\n",
      "    timelyFiles = getTimelyFilesFromHDFS(startTime, timeWindowInMinutes, measDir)\n",
      "    listCellStats = []\n",
      "    print \"Cell stats\"\n",
      "    for fn in timelyFiles:\n",
      "        cs = computeCellStatsForOneMeasFile(sparkMaster, isRealPCMD, fn)\n",
      "        listCellStats.extend(cs)\n",
      "    #\n",
      "    return cs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "   #stub from computeCellStatsForManyMeasfiles\n",
      "    sparkAppName = 'PCMDMiner' \n",
      "    sc = SparkContext(sparkMaster, sparkAppName)\n",
      "    print \"List Cell stats \", cs\n",
      "    rddCellStats = sc.parallelize(listCellStats)\n",
      "    cellStats = rddCellStats \\\n",
      "        .reduceByKey(lambda x, y: reduceToSingleStat(x, y)) \\\n",
      "        .sortByKey() \\\n",
      "        .collect()\n",
      "    sc.stop()\n",
      "    \n",
      "    return cellStats"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "startTime = '2014-05-12.12_00'\n",
      "timeWindowInMinutes = 2\n",
      "print startTime, timeWindowInMinutes\n",
      "\n",
      "\n",
      "if True:\n",
      "    \n",
      "    isRealPCMD, sparkMaster, measDir, caseId, rnpId = getBasicConfigInfo() \n",
      " \n",
      "    \n",
      "    print sparkMaster\n",
      "    \n",
      "    cellStats = computeCellStatsForManyMeasFiles(startTime, timeWindowInMinutes, isRealPCMD, sparkMaster, measDir)\n",
      "    #saveAsFile(\"output_cell_statistics\", cellStats, caseId, rnpId)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2014-05-12.12_00 2\n",
        "spark://192.168.122.1:7077\n",
        "Cell stats"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "ename": "ValueError",
       "evalue": "Cannot run multiple SparkContexts at once",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-6-a73183530eab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[0msparkMaster\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mcellStats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomputeCellStatsForManyMeasFiles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstartTime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeWindowInMinutes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0misRealPCMD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparkMaster\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeasDir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[1;31m#saveAsFile(\"output_cell_statistics\", cellStats, caseId, rnpId)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-5-5e052a691181>\u001b[0m in \u001b[0;36mcomputeCellStatsForManyMeasFiles\u001b[1;34m(startTime, timeWindowInMinutes, isRealPCMD, sparkMaster, measDir)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Cell stats\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mfn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtimelyFiles\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mcs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomputeCellStatsForOneMeasFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparkMaster\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0misRealPCMD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mlistCellStats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-4-c0d06d91b808>\u001b[0m in \u001b[0;36mcomputeCellStatsForOneMeasFile\u001b[1;34m(sparkMaster, isRealPCMD, measFileName)\u001b[0m\n\u001b[0;32m      5\u001b[0m                  \u001b[1;33m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparkAppName\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m                  .set('spark.scheduler.mode', 'FAIR'))\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msparkConf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mpcmdStrings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeasFileName\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mcellStats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpcmdStrings\u001b[0m        \u001b[1;33m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmapPCMDString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0misRealPCMD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m         \u001b[1;33m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mreduceToSingleStat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m         \u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/share/spark-0.9.0-incubating-bin-hadoop2/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf)\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \"\"\"\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menvironment\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menvironment\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/share/spark-0.9.0-incubating-bin-hadoop2/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance)\u001b[0m\n\u001b[0;32m    163\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cannot run multiple SparkContexts at once\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if __name__ == \"__main__\":\n",
      "\tif len(sys.argv) < 3:\n",
      "\t\tprint >> sys.stderr, \"Usage: generate_windowed_cell_statistics  start_time_string  time_in_minutes\"\n",
      "\t\tprint >> sys.stderr, \"Enter time in year-month-day.hr_min format . Everything but year is in 2 digits\"\n",
      "\t\texit(-1)\n",
      "\telse:\n",
      "\t\tstartTime = sys.argv[1]\n",
      "\t\ttimeWindowInMinutes = int(sys.argv[2])\n",
      "\t#########################################################\n",
      "\t# Unused commands:\n",
      "\t#sc = SparkContext(sparkMaster, \"PCMDMiner\")\n",
      "\t#cellStats = parallelyComputeCellStatsForManyMeasFiles(startTime, timeWindowInMinutes, isRealPCMD, sparkMaster, measDir)\n",
      "\t#saveToDatabase(cellStats, caseId, rnpId)\n",
      "\t#########################################################\n",
      "\t# use these three steps\n",
      "\tisRealPCMD, sparkMaster, measDir, caseId, rnpId = getBasicConfigInfo() \n",
      "\tcellStats = computeCellStatsForManyMeasFiles(startTime, timeWindowInMinutes, isRealPCMD, sparkMaster, measDir)\n",
      "\tprint \"########################  cellStats ####################################\", cellStats\n",
      "\t#saveAsFile(\"output_cell_statistics\", cellStats, caseId, rnpId)\n",
      "\t#createClusters(cellStats)\n",
      "\t#########################################################"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}