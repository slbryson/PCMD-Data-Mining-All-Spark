{
 "metadata": {
  "name": "",
  "signature": "sha256:c80a38709bd4f70b08ae059f17feea55bf08b0639a21b4ccf9cdcd474ccae8b4"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "spark_home = \"/usr/local/share/spark-0.9.0-incubating-bin-hadoop2\"\n",
      "os.environ['SPARK_HOME'] = spark_home\n",
      "import sys\n",
      "path_to_pyspark = os.path.join(spark_home,\"python\")\n",
      "sys.path.insert(0,path_to_pyspark)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyspark import SparkContext, SparkConf\n",
      "from collections import defaultdict\n",
      "import sys, operator, math, imp, os, uuid, ConfigParser, pyodbc\n",
      "import datetime as dt\n",
      "from multiprocessing import Pool\n",
      "import itertools as it\n",
      "import threading\n",
      "import networkx as nx\n",
      "import numpy \n",
      "import matplotlib . pyplot as plt\n",
      "#Constants for real pcmd string\n",
      "CONFIG_FILE = 'config.ini'\n",
      "FIELD_SERVING_CELL_ID_PRIMARY = 21\n",
      "FIELD_UE_LOCATION_CAPABILITY = 158\n",
      "FIELD_CELL_ID = 131\n",
      "START_CHAR_MEAS = '['\n",
      "END_CHAR_PCMD = '|'\n",
      "LENGTH_MEAS_DATA = 4\n",
      "FIELD_CELL_ID_MEAS = 2\n",
      "FIELD_RSRP_MEAS = 3\n",
      "RSRP_MIN = -140.0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import csv\n",
      "pcmdString = []\n",
      "with open('sample','rb') as csvfile:\n",
      "    ps = csv.reader(csvfile, delimiter =\";\")\n",
      "    \n",
      "    for row in ps:\n",
      "        pcmdString.append(row)\n",
      "#print pcmdString\n",
      "\n",
      "str =(pcmdString)\n",
      "print len(str)\n",
      " \n",
      "pcmdString[19][0]\n",
      "       "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 20\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "'6'"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def mapRealPCMDString(pcmdString):\n",
      "    servingCell, fieldDict = getFieldDictFromMeasRecord(pcmdString)\n",
      "    #debug\n",
      "    print \"We made it\"\n",
      "    if not fieldDict:\n",
      "\t\treturn []\n",
      "# Reset listOfMeasurements\n",
      "    listOfMeasurements = []\n",
      "    if servingCell in fieldDict.keys():\n",
      "\t\tserverDLRLC = fieldDict[servingCell][0][0]\n",
      "    else:\n",
      "\t\tserverDLRLC = -1\n",
      " \n",
      "    \n",
      "    for cellID in fieldDict.keys():\n",
      "        f1 = fieldDict[cellID][0][0]\n",
      "        f2 = fieldDict[cellID][0][1]\n",
      "        listOfMeasurements.append(((servingCell,cellID), \\\n",
      "\t\t\t(f1,f1*f1,serverDLRLC,serverDLRLC*serverDLRLC,f1*serverDLRLC,1)\t\\\n",
      "\t\t\t))\n",
      "\treturn listOfMeasurements"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "this function gets each field requested from the pcmd file\n",
      "\"\"\"\n",
      "def getFieldDictFromMeasRecord(pcmdString):\n",
      "    #assume this function is getting one record at a time\n",
      "    elementsOfPCMDString = pcmdString.split(\";\")\n",
      "    #elementsOfPCMDString =(pcmdString)\n",
      "              \n",
      "    cellID = elementsOfPCMDString[FIELD_CELL_ID -1]\n",
      "    servingCell = cellID\n",
      "    print cellID, secondaryFieldNum\n",
      "    secondaryFieldNum = elementsOfPCMDString[173-1]\n",
      "    \n",
      "    if secondaryFieldNum >= 2 and len(elementsOfPCMDString) >=299:\n",
      "        DLRLC_First = elementsOfPCMDString[285-1]\n",
      "    else:\n",
      "        print \"Shouldn't Hit this\"\n",
      "        return [], []\n",
      " \n",
      "    fieldDict =defaultdict(list)\n",
      "    fieldDict[cellID].append((float(DLRLC_First)))\n",
      "    return servingCell, fieldDict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "servingCell, fieldDict = getFieldDictFromMeasRecord(pcmdString[0])\n",
      "print servingCell, fieldDict\n",
      "servingCell, fieldDict = getFieldDictFromMeasRecord(pcmdString[8])\n",
      "print fieldDict\n",
      "for row in range(0, 8):\n",
      "    print row\n",
      "    servingCell, fieldDict = getFieldDictFromMeasRecord(pcmdString[row])\n",
      "    print fieldDict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "AttributeError",
       "evalue": "'list' object has no attribute 'split'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-16-738c78e0c1f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mservingCell\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfieldDict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetFieldDictFromMeasRecord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpcmdString\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mservingCell\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfieldDict\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mservingCell\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfieldDict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetFieldDictFromMeasRecord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpcmdString\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mfieldDict\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-14-fdfd6d0f18df>\u001b[0m in \u001b[0;36mgetFieldDictFromMeasRecord\u001b[1;34m(pcmdString)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgetFieldDictFromMeasRecord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpcmdString\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m#assume this function is getting one record at a time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0melementsOfPCMDString\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpcmdString\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\";\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;31m#elementsOfPCMDString =(pcmdString)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(fieldDict), fieldDict.items()\n",
      " \n",
      "if True:\n",
      "    isRealPCMD, sparkMaster, measDir, caseId, rnpId = getBasicConfigInfo() \n",
      "    print sparkMaster\n",
      "sparkAppName = 'PCMDMiner' + uuid.uuid1().hex\n",
      "sparkConf = (SparkConf()\n",
      "\t       .setMaster(sparkMaster)\n",
      "    \t       .setAppName(sparkAppName)\n",
      "    \t       .set('spark.scheduler.mode', 'FAIR'))\n",
      "\n",
      "#sc = SparkContext(conf = sparkConf)\n",
      "pcmdStrings = sc.textFile('sample')\n",
      "\n",
      "cellStats = pcmdStrings\\\n",
      "\t\t.flatMap(lambda x: mapRealPCMDString(x)) \\\n",
      "\t    .reduceByKey(lambda x, y: reduceToSingleStat(x, y)) \\\n",
      "\t\t.collect()\n",
      "sc.stop()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "Py4JJavaError",
       "evalue": "An error occurred while calling o257.collect.\n: org.apache.spark.SparkException: Job aborted: Task 15.0:0 failed 1 times (most recent failure: Exception failure: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/share/spark-0.9.0-incubating-bin-hadoop2/python/pyspark/worker.py\", line 77, in main\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/share/spark-0.9.0-incubating-bin-hadoop2/python/pyspark/rdd.py\", line 1022, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/share/spark-0.9.0-incubating-bin-hadoop2/python/pyspark/rdd.py\", line 1022, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/share/spark-0.9.0-incubating-bin-hadoop2/python/pyspark/rdd.py\", line 194, in func\n    def func(s, iterator): return f(iterator)\n  File \"/usr/local/share/spark-0.9.0-incubating-bin-hadoop2/python/pyspark/rdd.py\", line 870, in combineLocally\n    for x in iterator:\n  File \"<ipython-input-33-0239f26f1c3e>\", line 15, in <lambda>\n  File \"<ipython-input-3-face1b850eaa>\", line 2, in mapRealPCMDString\n  File \"<ipython-input-28-fdfd6d0f18df>\", line 11, in getFieldDictFromMeasRecord\nUnboundLocalError: local variable 'secondaryFieldNum' referenced before assignment\n)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1028)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1026)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$abortStage(DAGScheduler.scala:1026)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:619)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:619)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:619)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$start$1$$anon$2$$anonfun$receive$1.applyOrElse(DAGScheduler.scala:207)\n\tat akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)\n\tat akka.actor.ActorCell.invoke(ActorCell.scala:456)\n\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:219)\n\tat akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-33-0239f26f1c3e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mpcmdStrings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'sample'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mcellStats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpcmdStrings\u001b[0m         \u001b[1;33m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmapRealPCMDString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m            \u001b[1;33m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mreduceToSingleStat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m                 \u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/share/spark-0.9.0-incubating-bin-hadoop2/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    461\u001b[0m         \"\"\"\n\u001b[0;32m    462\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0m_JavaStackTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mst\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 463\u001b[1;33m           \u001b[0mbytesInJava\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    464\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_collect_iterator_through_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbytesInJava\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/share/spark-0.9.0-incubating-bin-hadoop2/python/lib/py4j-0.8.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    535\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[1;32m--> 537\u001b[1;33m                 self.target_id, self.name)\n\u001b[0m\u001b[0;32m    538\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/share/spark-0.9.0-incubating-bin-hadoop2/python/lib/py4j-0.8.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    299\u001b[0m                     \u001b[1;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[0;32m    301\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                 raise Py4JError(\n",
        "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o257.collect.\n: org.apache.spark.SparkException: Job aborted: Task 15.0:0 failed 1 times (most recent failure: Exception failure: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/share/spark-0.9.0-incubating-bin-hadoop2/python/pyspark/worker.py\", line 77, in main\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/share/spark-0.9.0-incubating-bin-hadoop2/python/pyspark/rdd.py\", line 1022, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/share/spark-0.9.0-incubating-bin-hadoop2/python/pyspark/rdd.py\", line 1022, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/share/spark-0.9.0-incubating-bin-hadoop2/python/pyspark/rdd.py\", line 194, in func\n    def func(s, iterator): return f(iterator)\n  File \"/usr/local/share/spark-0.9.0-incubating-bin-hadoop2/python/pyspark/rdd.py\", line 870, in combineLocally\n    for x in iterator:\n  File \"<ipython-input-33-0239f26f1c3e>\", line 15, in <lambda>\n  File \"<ipython-input-3-face1b850eaa>\", line 2, in mapRealPCMDString\n  File \"<ipython-input-28-fdfd6d0f18df>\", line 11, in getFieldDictFromMeasRecord\nUnboundLocalError: local variable 'secondaryFieldNum' referenced before assignment\n)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1028)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1026)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$abortStage(DAGScheduler.scala:1026)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:619)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:619)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:619)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$start$1$$anon$2$$anonfun$receive$1.applyOrElse(DAGScheduler.scala:207)\n\tat akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)\n\tat akka.actor.ActorCell.invoke(ActorCell.scala:456)\n\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:219)\n\tat akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 [('311:480:131ac02', [65.0])]\n",
        "spark://192.168.122.1:7077\n"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Created by Supratim Deb on March 20, 2014\n",
      "\"\"\"\n",
      "This is a pyspark code for generating cell statistics from PCMD data\n",
      "\"\"\"\n",
      "\n",
      "from pyspark import SparkContext, SparkConf\n",
      "from collections import defaultdict\n",
      "import sys, operator, math, imp, os, uuid, ConfigParser, pyodbc\n",
      "import datetime as dt\n",
      "from multiprocessing import Pool\n",
      "import itertools as it\n",
      "import threading\n",
      "import networkx as nx\n",
      "import matplotlib . pyplot as plt\n",
      "\n",
      "#Max cores per worker\n",
      "SPARK_CORES_MAX = 3\n",
      "\n",
      "\n",
      "#Constants for synthetic pcmd string\n",
      "STRING_INDEX_FIRST_CELL = 4\n",
      "STRING_INDEX_NUM_CELLS = 3\n",
      "NUM_MEAS_STATISTICS = 6\n",
      "\n",
      "#Constants for real pcmd string\n",
      "CONFIG_FILE = 'config.ini'\n",
      "FIELD_SERVING_CELL_ID_PRIMARY = 21\n",
      "FIELD_UE_LOCATION_CAPABILITY = 158\n",
      "FIELD_CELL_ID = 131\n",
      "START_CHAR_MEAS = '['\n",
      "END_CHAR_PCMD = '|'\n",
      "LENGTH_MEAS_DATA = 4\n",
      "FIELD_CELL_ID_MEAS = 2\n",
      "FIELD_RSRP_MEAS = 3\n",
      "RSRP_MIN = -140.0\n",
      "DATETIME_FORMAT_PCMD_FILENAME = '%Y-%m-%d.%H_%M'\n",
      "INT_THRESHOLD = 0.09\n",
      "\n",
      "\n",
      "#Constant strings for outpul file string\n",
      "MEAS_STRINGS = ['MeanServer', 'MeanNeighbor', 'VarianceServer', 'CovarianceNeighbor', 'VarianceNeighbor']\n",
      "STRING_AVG_SERVER_RSRP = 'MeanServer'\n",
      "STRING_VAR_SERVER_RSRP = 'VarianceServer'\n",
      "STRING_AVG_NGHBR_RSRP = 'MeanNeighbor'\n",
      "STRING_VAR_NGHBR_RSRP = 'VarianceNeighbor'\n",
      "STRING_COVAR_RSRP = 'CovarianceNeighbor'\n",
      "\n",
      "\n",
      "#def computeCellStatsForManyMeasFiles(startTime, timeWindowInMinutes, isRealPCMD, sparkMaster, measDir):\n",
      "def parallelyComputeCellStatsForManyMeasFiles(startTime, timeWindowInMinutes, isRealPCMD, sparkMaster, measDir):\n",
      "\t\n",
      "\ttimelyFiles = getTimelyFilesFromHDFS(startTime, timeWindowInMinutes, measDir)\n",
      "\tlistCellStats = []\n",
      "\n",
      "\tpool = Pool()\n",
      "\tfileCellStats = pool.map(computeCellStatsForOneMeasFile_star, \\\n",
      "\t\t\t\t\t\t\t it.izip(it.repeat(sparkMaster), it.repeat(isRealPCMD), timelyFiles))\n",
      "\tpool.close()\n",
      "\tpool.join()\n",
      "\tlistCellStats = list(it.chain(*fileCellStats))\n",
      "\n",
      "\tsparkAppName = 'PCMDMiner' \n",
      "\tsc = SparkContext(sparkMaster, sparkAppName)\n",
      "\trddCellStats = sc.parallelize(listCellStats)\n",
      "\tcellStats = rddCellStats \\\n",
      "\t\t\t\t.reduceByKey(lambda x, y: reduceToSingleStat(x, y)) \\\n",
      "\t            .map(lambda x: (x[0], toCovarMatrix(x[1]))) \\\n",
      "\t            .sortByKey() \\\n",
      "\t            .collect()\n",
      "\tsc.stop()\n",
      "\n",
      "\treturn cellStats\n",
      "\n",
      "\n",
      "def computeCellStatsForManyMeasFiles(startTime, timeWindowInMinutes, isRealPCMD, sparkMaster, measDir):\n",
      "\ttimelyFiles = getTimelyFilesFromHDFS(startTime, timeWindowInMinutes, measDir)\n",
      "\tlistCellStats = []\n",
      "\tfor fn in timelyFiles:\n",
      "\t\tcs = computeCellStatsForOneMeasFile(sparkMaster, isRealPCMD, fn)\n",
      "\t\tlistCellStats.extend(cs)\n",
      "\tsparkAppName = 'PCMDMiner' \n",
      "\treturn listCellStats\n",
      "\tsc = SparkContext(sparkMaster, sparkAppName)\n",
      "\trddCellStats = sc.parallelize(listCellStats)\n",
      "\tcellStats = rddCellStats \\\n",
      "\t\t\t.reduceByKey(lambda x, y: reduceToSingleStat(x, y)) \\\n",
      "\t\t\t.sortByKey() \\\n",
      "\t\t\t.collect()\n",
      "\tsc.stop()\n",
      "\n",
      "\treturn cellStats\n",
      "\n",
      "\n",
      "\"\"\"\n",
      "the following is a wrapper around computeCellStatsForOneMeasFile with all arguments\n",
      "put in a single list. this is required for parrallel processing of multiple files.\n",
      "\"\"\"\n",
      "def computeCellStatsForOneMeasFile_star(params):\n",
      "\treturn(computeCellStatsForOneMeasFile(*params))\n",
      "\n",
      "\n",
      "def computeCellStatsForOneMeasFile(sparkMaster, isRealPCMD, measFileName):\n",
      "\tsparkAppName = 'PCMDMiner' + uuid.uuid1().hex\n",
      "\tsparkConf = (SparkConf()\n",
      "\t       .setMaster(sparkMaster)\n",
      "    \t       .setAppName(sparkAppName)\n",
      "    \t       .set('spark.scheduler.mode', 'FAIR'))\n",
      "\n",
      "\tsc = SparkContext(conf = sparkConf)\n",
      "\tpcmdStrings = sc.textFile(measFileName)\n",
      "#\tcellStats = pcmdStrings\\\n",
      "#\t            .flatMap(lambda x: mapPCMDString(isRealPCMD, x)) \\\n",
      "#\t            .reduceByKey(lambda x, y: reduceToSingleStat(x, y)) \\\n",
      "#\t            .collect()\n",
      "\tcellStats = pcmdStrings\\\n",
      "\t\t.flatMap(lambda x: mapPCMDString(isRealPCMD, x)) \\\n",
      "\t        .reduceByKey(lambda x, y: reduceToSingleStat(x, y)) \\\n",
      "\t\t.collect()\n",
      "\tsc.stop()\n",
      "\t#debug\n",
      "\t            \n",
      "\treturn cellStats\n",
      "\n",
      "\n",
      "\n",
      "def getTimelyFilesFromHDFS(startTime, timeWindowInMinutes, measDirName):\n",
      "\thadoopCommand = 'hadoop fs -ls %s/' % measDirName\n",
      "\tfullListings = os.popen(hadoopCommand).read().split('\\n')\n",
      "\tfileNames = [x.split(' ')[-1].split('/')[-1] for x in fullListings][1:-1]\n",
      "\ttimelyFiles = []\n",
      "\tdtStartTime = dt.datetime.strptime(startTime, DATETIME_FORMAT_PCMD_FILENAME)\n",
      "\tdtEndTime = dtStartTime + dt.timedelta(0, timeWindowInMinutes * 60)\n",
      "\tfor fn in fileNames:\n",
      "\t\tfileCreateTimeString = '.'.join(fn.split('.')[:2])\n",
      "\t\tdtFileCreateTime = dt.datetime.strptime(fileCreateTimeString, DATETIME_FORMAT_PCMD_FILENAME)\n",
      "\t\tif ((dtFileCreateTime >= dtStartTime) and (dtFileCreateTime <= dtEndTime)):\n",
      "\t\t\tif (measDirName[-1] != '/'):\n",
      "\t\t\t\tsep = '/'\n",
      "\t\t\telse:\n",
      "\t\t\t\tsep = ''\n",
      "\t\t\ttimelyFiles.append(measDirName + sep + fn)\n",
      "\t\t\n",
      "\treturn timelyFiles\n",
      "\n",
      "\n",
      "\n",
      "def mapPCMDString(isRealPCMD, pcmdString):\n",
      "\tif isRealPCMD:\n",
      "\t\treturn mapRealPCMDString(pcmdString)\n",
      "\telse:\n",
      "\t\treturn mapSyntheticPCMDString(pcmdString)\n",
      "\n",
      "\n",
      "\n",
      "\"\"\"\n",
      "The following map function creates (key, value) pairs where \n",
      "key = (serving_cell, neighbor_cell) which is 2-tuple\n",
      "value = (server_RSRP, nghbr_RSRP, server_RSRP^2, server_RSRP*nghbr_RSRP, nghbr_RSRP^2, 1)\n",
      "which is a 5-tuple.\n",
      "This representation helps us aggregate the values in the reduce function.\n",
      "\"\"\"\n",
      "def mapSyntheticPCMDString(pcmdString):\n",
      "\telementsOfPCMDString = pcmdString.split(\",\")\n",
      "\tservingCell = elementsOfPCMDString[0]\n",
      "\tserverRSRP = max(0, float(elementsOfPCMDString[STRING_INDEX_FIRST_CELL + 1]) - RSRP_MIN)\n",
      "\tlistOfMeasurements = []\n",
      "\tfor count in range(0,int(elementsOfPCMDString[STRING_INDEX_NUM_CELLS])):\n",
      "\t\tnghbrCell = elementsOfPCMDString[STRING_INDEX_FIRST_CELL + 2*count]\n",
      "\t\tnghbrRSRP = max(0, float(elementsOfPCMDString[STRING_INDEX_FIRST_CELL + 2*count + 1]) -RSRP_MIN)\n",
      "\t\tlistOfMeasurements.append((\n",
      "\t\t\t(servingCell, nghbrCell),\\\n",
      "\t\t\t(serverRSRP, nghbrRSRP, serverRSRP*serverRSRP,\\\n",
      "\t\t\t serverRSRP*nghbrRSRP, nghbrRSRP*nghbrRSRP, 1)\\\n",
      "\t\t\t))\n",
      "\n",
      "\treturn listOfMeasurements\n",
      "\n",
      "\"\"\"\n",
      "this function retrieves all measurement strings from a pcmd string \n",
      "\"\"\"\n",
      "def getListOfMeasurementRecordsFromPCMDString(pcmdString):\n",
      "\telementsOfPCMDString = pcmdString.split(\";\")\n",
      "\tservingCell = elementsOfPCMDString[FIELD_SERVING_CELL_ID_PRIMARY - 1]\n",
      "\t#RSRP is just one field needed.\n",
      "\tif START_CHAR_MEAS not in pcmdString:\n",
      "\t\treturn servingCell, []\n",
      "\n",
      "\tstartOfMeasRecords = pcmdString.index(START_CHAR_MEAS) + 1\n",
      "\tendOfMeasRecords = pcmdString.index(END_CHAR_PCMD)\n",
      "\tallMeasRecords = (pcmdString[startOfMeasRecords : endOfMeasRecords]).split(\";\")\n",
      "\tlistOfMeasRecords = []\n",
      "\tcount = 0\n",
      "\twhile (count < len(allMeasRecords)):\n",
      "\t\tstartOfThisMeasRecord = count\n",
      "\t\tlengthOfThisMeasRecord = int(allMeasRecords[count]) * LENGTH_MEAS_DATA + 1\n",
      "\t\tendOfThisMeasRecord = startOfThisMeasRecord + lengthOfThisMeasRecord\n",
      "\t\tlistOfMeasRecords.append(allMeasRecords[startOfThisMeasRecord : endOfThisMeasRecord])\n",
      "\t\tcount = endOfThisMeasRecord\n",
      "\t#Add an additional return value that will include additional PCMD fields\n",
      "\t#Need to correct the syntax to combine these two structures into the listOfllMeasRecords\n",
      "\treturn servingCell, listOfMeasRecords\n",
      "\n",
      "\n",
      "\"\"\"\n",
      "The following function reads a pcmd measurement string and retrieves the values in\n",
      "a dictionary format with keys being cellids and values being RSRPs \n",
      "\"\"\"\n",
      "def getRSRPDictFromMeasRecord(servingCell, measRecord):\n",
      "\tnumCellsInMeasRecord = int(measRecord[0])\n",
      "\tnumMeasData = int((len(measRecord) - 1)/ LENGTH_MEAS_DATA)\n",
      "\t\"\"\" create a dictionary with rsrp measurements in this record \"\"\"\n",
      "\tindex = 0\n",
      "\trsrpMeasVal = defaultdict(float) \n",
      "\n",
      "\tfor count in range(0, numMeasData):\n",
      "\t\tthisMeasData = measRecord[1+count*LENGTH_MEAS_DATA : 1+(count + 1)*LENGTH_MEAS_DATA]\n",
      "\t\tthisCell = thisMeasData[FIELD_CELL_ID_MEAS - 1]\n",
      "\t\t#Apparently the RSRP was stored as a string as well and needs to be converted\n",
      "\t\tthisRSRPString = thisMeasData[FIELD_RSRP_MEAS - 1]\n",
      "\t\tif (thisCell == '') or (thisRSRPString == ''):\n",
      "\t\t\tcontinue\n",
      "\t\telse:\n",
      "\t\t\t#This line creates a dictionary based on the cell found in the measurement data. \n",
      "\t\t\trsrpMeasVal[thisCell] = float(thisRSRPString)\n",
      "\t#Note servingCell was passed to this function but not used.\n",
      "\treturn rsrpMeasVal\n",
      "\n",
      "\n",
      "\"\"\"\n",
      "define a commutative and associative function for combining aggregate statistics\n",
      "this is used by reduce function in the code later\n",
      "\"\"\"\n",
      "def reduceToSingleStat(stat1, stat2):\n",
      "\tif stat2.share[1] < NUM_MEAS_STATISTICS -1:\n",
      "\t\treturn []\n",
      "\tsumMeasCount = stat1[NUM_MEAS_STATISTICS-1] + stat2[NUM_MEAS_STATISTICS-1]\n",
      "\twtStat1 = 1.0 * stat1[NUM_MEAS_STATISTICS-1] / sumMeasCount\n",
      "\twtStat2 = 1.0 * stat2[NUM_MEAS_STATISTICS-1] / sumMeasCount\n",
      "\treducedStat = [0] * NUM_MEAS_STATISTICS\n",
      "\tfor index in range(0, NUM_MEAS_STATISTICS - 1):\n",
      "\t\treducedStat[index] = wtStat1 * stat1[index] + wtStat2 * stat2[index]\n",
      "\treducedStat[NUM_MEAS_STATISTICS-1] = sumMeasCount\n",
      "\treturn tuple(reducedStat)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def toCovarMatrix(stat):\n",
      "\tparams = [0] * NUM_MEAS_STATISTICS\n",
      "\tfor index in [0, 1, NUM_MEAS_STATISTICS-1]:\n",
      "\t\tparams[index] = stat[index]\n",
      "\tparams[2] = stat[2] - stat[0]*stat[0]\n",
      "\tparams[3] = stat[3] - stat[0]*stat[1]\n",
      "\tparams[4] = stat[4] - stat[1]*stat[1]\n",
      "\treturn tuple(params)\n",
      "\n",
      "\n",
      "def saveAsFile(outputFile, cellStats, caseId, rnpId):\n",
      "\twith open(outputFile + '.txt', 'w') as opf:\n",
      "\t\tfor cs in cellStats:\n",
      "\t\t\top_string = '{:16s}  {:16s}   '.format(cs[0][0], cs[0][1])\n",
      "\t\t\tfor i in range(0, NUM_MEAS_STATISTICS - 1):\n",
      "\t\t\t\top_string =  op_string + '{:>10s}'.format('{:4.2f}'.format(cs[1][i]))\n",
      "\t\t\top_string =  op_string +\\\n",
      "\t\t\t\t '{:>8s}'.format('{:4d}'.format(cs[1][NUM_MEAS_STATISTICS-1]))\n",
      "\t\t\topf.write(op_string + '\\n')\n",
      "\n",
      "\n",
      "\tallDBRowsAsStrings = getAllDBRowsAsStrings(cellStats, caseId, rnpId)\n",
      "\twith open(outputFile + '.csv', 'w') as opf:\n",
      "\t\tfor measString in allDBRowsAsStrings:\n",
      "\t\t\topf.write(measString + '\\n')\n",
      "\n",
      "\treturn\n",
      "\n",
      "\n",
      "def getAllDBRowsAsStrings(cellStats, caseId, rnpId):\n",
      "\tallDBRowsAsStrings = []\n",
      "\tfor cs in cellStats:\n",
      "\t\tcommonString = \"%d, %d\" % (caseId, rnpId)\n",
      "\t\tmeasStats = [RSRP_MIN + cs[1][0], RSRP_MIN + cs[1][1], cs[1][2], cs[1][3], cs[1][4], cs[1][5]]\n",
      "\t\tfor i in range(len(MEAS_STRINGS)):\n",
      "\t\t\tmeasType = MEAS_STRINGS[i]\n",
      "\t\t\tmeasuredCells= (cs[0][0], cs[0][1])\n",
      "\t\t\tmeasString = \"%s, '%s', '%s', '%s', '%s', %s, %s\"\\\n",
      "\t\t\t             % (commonString, measType, cs[0][0], measuredCells[0], measuredCells[1],\n",
      "\t\t\t                str(measStats[i]), str(measStats[5]))\n",
      "\t\t\tallDBRowsAsStrings.append('(' + measString + ')')\n",
      "\treturn allDBRowsAsStrings\n",
      "\n",
      "\n",
      "def saveToDatabase(cellStats, caseId, rnpId):\n",
      "\tconStr = getDBConnStringConfig()\n",
      "\tcnx = pyodbc.connect(conStr)\n",
      "\tcursor = cnx.cursor()\n",
      "\tcnf = ConfigParser.ConfigParser()\n",
      "\tcnf.read(CONFIG_FILE)\n",
      "\ttableName = dict(cnf.items('DB_TABLE'))['name']\n",
      "\ttableOverwrite = dict(cnf.items('DB_TABLE'))['overwrite']\n",
      "\tif tableOverwrite == 'y':\n",
      "\t\tdelStr = 'DELETE FROM %s WHERE Case_Id = %d AND RNP_Simulation_Id = %d' % (tableName, caseId, rnpId)\n",
      "\t\tcursor.execute(delStr)\n",
      "\t\tprint ('Deleted %d from DB: ' % cursor.rowcount) + delStr\n",
      "\t\tcnx.commit()\n",
      "\n",
      "\n",
      "\tallDBRowsAsStrings = getAllDBRowsAsStrings(cellStats, caseId, rnpId)\n",
      "\tfor dbRowString in allDBRowsAsStrings:\n",
      "\t\tinsertStr = \"INSERT INTO RSRPJointGaussianParams VALUES %s\" % dbRowString\n",
      "\t\tcursor.execute(insertStr)\n",
      "\tcnx.commit()\n",
      "\n",
      "\tcnx.close()\n",
      "\treturn\n",
      "\n",
      "\n",
      "def getDBConnStringConfig():\n",
      "\tconStr = r''\n",
      "\tcnf = ConfigParser.ConfigParser()\n",
      "\tcnf.read(CONFIG_FILE)\n",
      "\tfor option in cnf.items('DATABASE'):\n",
      "\t\tconStr += '%s=%s;' %  option\n",
      "\treturn conStr.rstrip(';')\n",
      "\n",
      "\n",
      "def getBasicConfigInfo():\n",
      "\tcnf = ConfigParser.ConfigParser()\n",
      "\tcnf.read(CONFIG_FILE)\n",
      "\tisRealPCMD = cnf.getint('BASIC', 'REAL_PCMD')\n",
      "\tsparkMaster = cnf.get('BASIC', 'SPARK_MASTER')\n",
      "\tmeasDir = cnf.get('BASIC', 'HADOOP_PCMD_DIR')\n",
      "\tcaseId = cnf.getint('BASIC', 'CASE_ID')\n",
      "\trnpId = cnf.getint('BASIC', 'RNP_SIM_ID')\n",
      "\treturn isRealPCMD, sparkMaster, measDir, caseId, rnpId\n",
      "\n",
      "\n",
      "def createClusters(cellStats):\n",
      "\tcellGraph = nx.DiGraph()\n",
      "\tcellNodes =set([])\n",
      "\tcellEdges = []\n",
      "\tfor cs in cellStats:\n",
      "\t\tcellNodes = cellNodes.union(set(cs[0]))\n",
      "\n",
      "\tcellTotalWeight = defaultdict(int)\n",
      "\tfor node in cellNodes:\n",
      "\t\tcellTotalWeight[node] = sum([x[1][5] for x in cellStats if x[0][0] == node and x[0][1] != node])\n",
      "\t\t#print '%s : %d' % (node, cellTotalWeight[node])\n",
      "\t\t#sys.stdin.read(1)\n",
      "\n",
      "\tfor cs in cellStats:\n",
      "\t\tif ((cs[0][0] != cs[0][1]) and (cs[1][5] > cellTotalWeight[cs[0][0]] * INT_THRESHOLD)):\n",
      "\t\t\tcellEdges.append(cs[0])\n",
      "\t\t\t#cellEdges.append((cs[0][0], cs[0][1], cs[1][5]))\n",
      "\n",
      "\tcellGraph.add_nodes_from(list(cellNodes))\n",
      "\tcellGraph.add_edges_from(cellEdges)\n",
      "\tcomps = nx.connected_component_subgraphs(cellGraph.to_undirected())\n",
      "\tprint 'Nodes = %d, Edges = %d' % (cellGraph.number_of_nodes(), cellGraph.number_of_edges())\n",
      "\tind = 0\n",
      "\tfor sg in comps:\n",
      "\t\tind += 1\n",
      "\t\tprint '%d : %d , %d' % (ind, sg.number_of_nodes(), sg.number_of_edges())\n",
      "\n",
      "\treturn\n",
      "\n",
      "\n",
      "\t#########################################################\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}